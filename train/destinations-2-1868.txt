Oh dear oh dear oh dear!!! All sounds a bit ironic to me! Small group of colonies on the other side of the planet trying to free themselves from the influence of a powerful, arrogant self riteous empire. Only to become the very thing they were trying to free themselves from. Don't get me wrong, 200 years ago America was the best place in the world, a haven for those fleeing oppression and war. Unfortunately, since then America has never really had to face up to any of the immoral things that they've done, in the same way that most of Europe has for example. As a nation they've never had to say 'That was wrong and we are truly sorry'. If I was being crass I'ld say they have no national health service, they're undereducated and they shoot each other. If I was being more sensible I'ld say alot of America's social problems as well as it's international image are a result of the long term effects of right wing politics (blind patriotism included). If America was a film it would be a glossy Blockbuster, heavily marketed with lots of special effects, 2-dimensional, trite plot with plenty of bad acting that leaves you feeling uninspired and thoroughly disappointed. Power corrupts, absolute power corrupts absolutely. Thank you and have a nice day!