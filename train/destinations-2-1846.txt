I'm sure the US is full of nice and friendly people, but as for the prevailing culture, the prevailing morals (far from being Christian by the way, even though many Christian Americans seem to think so) and the political policies, I can't stand it. The culture is streamlined and dulling of the intellect, most noteably in popular fields such as film and popular music. While there is of course great diversity in a country with nearly 300 million inhabitants, the moral values of most Americans I've encountered seem to me highly hypocritical and unsympathetic. In essence it is the ethics of the self and of self-righteousness; you (and your family) are more important than everybody else, even to the point where it's ok that others die en masse if that means you can buy more useless crap or feel safer or whatnot. Now this attitude, I don't like (and I'm sure many Americans don't either). At the heart of American politics is money (which makes sense considering the culture's emphasis on the self), not democracy or freedom or whatever. The political record of the US is rather ugly (though not nearly as ugly as that of many European contries, which, on the other hand, has been around for a lot longer). I won't try to build an argument supporting this seeing how the US's crimes are (should be) common knowledge. Nevertheless, because of all the warmongers here, I can't resist stating that: the US did NOT put an end to Nazi Germany (military speaking, Soviet did), and they only entered WWII out of -surprise surprise- self interest.